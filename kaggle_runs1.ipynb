{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install asttokens==2.4.1\n",
    "%pip install certifi==2024.2.2\n",
    "%pip install charset-normalizer==3.3.2\n",
    "%pip install click==8.1.7\n",
    "%pip install colorama==0.4.6\n",
    "%pip install comm==0.2.2\n",
    "%pip install contourpy==1.2.1\n",
    "%pip install cycler==0.12.1\n",
    "%pip install debugpy==1.8.1\n",
    "%pip install decorator==5.1.1\n",
    "%pip install docker-pycreds==0.4.0\n",
    "%pip install executing==2.0.1\n",
    "%pip install filelock==3.14.0\n",
    "%pip install fonttools==4.52.4\n",
    "%pip install fsspec==2024.5.0\n",
    "%pip install gitdb==4.0.11\n",
    "%pip install GitPython==3.1.43\n",
    "%pip install idna==3.7\n",
    "%pip install intel-openmp==2021.4.0\n",
    "%pip install ipykernel==6.29.4\n",
    "%pip install ipython==8.24.0\n",
    "%pip install jedi==0.19.1\n",
    "%pip install Jinja2==3.1.4\n",
    "%pip install joblib==1.4.2\n",
    "%pip install jupyter_client==8.6.2\n",
    "%pip install jupyter_core==5.7.2\n",
    "%pip install kiwisolver==1.4.5\n",
    "%pip install MarkupSafe==2.1.5\n",
    "%pip install matplotlib==3.9.0\n",
    "%pip install matplotlib-inline==0.1.7\n",
    "%pip install mkl==2021.4.0\n",
    "%pip install mpmath==1.3.0\n",
    "%pip install nest-asyncio==1.6.0\n",
    "%pip install networkx==3.3\n",
    "%pip install numpy==1.26.4\n",
    "%pip install opencv-python==4.9.0.80\n",
    "%pip install packaging==24.0\n",
    "%pip install parso==0.8.4\n",
    "%pip install pillow==10.3.0\n",
    "%pip install platformdirs==4.2.2\n",
    "%pip install prompt_toolkit==3.0.45\n",
    "%pip install protobuf==4.25.3\n",
    "%pip install psutil==5.9.8\n",
    "%pip install pure-eval==0.2.2\n",
    "%pip install Pygments==2.18.0\n",
    "%pip install pyparsing==3.1.2\n",
    "%pip install python-dateutil==2.9.0.post0\n",
    "%pip install pywin32==306\n",
    "%pip install PyYAML==6.0.1\n",
    "%pip install pyzmq==26.0.3\n",
    "%pip install requests==2.32.2\n",
    "%pip install scikit-learn==1.5.0\n",
    "%pip install scipy==1.13.1\n",
    "%pip install sentry-sdk==2.3.1\n",
    "%pip install setproctitle==1.3.3\n",
    "%pip install six==1.16.0\n",
    "%pip install smmap==5.0.1\n",
    "%pip install stack-data==0.6.3\n",
    "%pip install sympy==1.12\n",
    "%pip install tbb==2021.12.0\n",
    "%pip install threadpoolctl==3.5.0\n",
    "%pip install torch==2.3.0\n",
    "%pip install torchvision==0.18.0\n",
    "%pip install tornado==6.4\n",
    "%pip install traitlets==5.14.3\n",
    "%pip install typing_extensions==4.12.0\n",
    "%pip install urllib3==2.2.1\n",
    "%pip install wandb==0.17.0\n",
    "%pip install wcwidth==0.2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import wandb\n",
    "from torchvision import transforms\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import os\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from wandb import Api\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision.models as models\n",
    "# from utils import DiaMOSDataset, DiaMOSDataset_Cartesian, format_time, flat_accuracy, EarlyStopping, animal_version_name\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "#### utils.py\n",
    "\n",
    "animal_names = [\n",
    "    \"Aardvark\", \"Albatross\", \"Anteater\", \"Armadillo\",\n",
    "    \"Baboon\", \"Beaver\", \"Bison\", \"Buffalo\",\n",
    "    \"Camel\", \"Cheetah\", \"Cougar\", \"Chinchilla\",\n",
    "    \"Dingo\", \"Dolphin\", \"Duck\", \"Deer\",\n",
    "    \"Elephant\", \"Echidna\", \"Emu\", \"Eagle\",\n",
    "    \"Ferret\", \"Flamingo\", \"Fox\", \"Falcon\",\n",
    "    \"Giraffe\", \"Gazelle\", \"Gecko\", \"Gorilla\",\n",
    "    \"Hedgehog\", \"Hamster\", \"Hummingbird\", \"Heron\",\n",
    "    \"Iguana\", \"Impala\", \"Ibis\", \"Inchworm\",\n",
    "    \"Jackal\", \"Jaguar\", \"Jackrabbit\", \"Jellyfish\",\n",
    "    \"Kangaroo\", \"Koala\", \"Kudu\", \"Kinkajou\",\n",
    "    \"Lemur\", \"Lynx\", \"Llama\", \"Leopard\",\n",
    "    \"Meerkat\", \"Mongoose\", \"Mole\", \"Manatee\",\n",
    "    \"Narwhal\", \"Numbat\", \"Newt\", \"Nightingale\",\n",
    "    \"Ocelot\", \"Ostrich\", \"Orangutan\", \"Octopus\",\n",
    "    \"Penguin\", \"Platypus\", \"Porcupine\", \"Puffin\",\n",
    "    \"Quokka\", \"Quail\", \"Quetzal\", \"Quoll\",\n",
    "    \"Raccoon\", \"Rabbit\", \"Raven\", \"Rhinoceros\",\n",
    "    \"Salamander\", \"Squirrel\", \"Seal\", \"Swan\",\n",
    "    \"Tapir\", \"Toucan\", \"Tortoise\", \"Turtle\",\n",
    "    \"Umbrellabird\", \"Urial\", \"Uakari\", \"Urchin\",\n",
    "    \"Vulture\", \"Viper\", \"Vicuna\", \"Vicu√±a\",\n",
    "    \"Wallaby\", \"Wombat\", \"Walrus\", \"Woodpecker\",\n",
    "    \"Xerus\", \"Xenops\", \"Xantus\", \"Xiphias\",\n",
    "    \"Yak\", \"Yabby\", \"Yellowjacket\", \"Yakutian\",\n",
    "    \"Zebra\", \"Zebu\", \"Zorilla\"\n",
    "]\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = np.argmax(labels, axis=1).flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def animal_version_name(project_name):\n",
    "    api = Api()\n",
    "    runs = api.runs(\"plant_disease_detection/\" + project_name)\n",
    "    animal_index = len(runs)\n",
    "    return animal_names[animal_index % len(animal_names)]\n",
    "\n",
    "\n",
    "class DiaMOSDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, data_path, transform=None, imputation_value=-1):\n",
    "        self.data = []\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.imputation_value = imputation_value\n",
    "        \n",
    "        csv_file_path = os.path.join(data_path, 'annotation/csv', csv_file) \n",
    "        \n",
    "        with open(csv_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            header = lines[0].strip().split(';')\n",
    "            for line in lines[1:]:\n",
    "                datapoint = dict(zip(header, line.strip().split(';')))\n",
    "                \n",
    "                disease = []\n",
    "                disease.append(int(datapoint['healthy']))\n",
    "                disease.append(int(datapoint['pear_slug']))\n",
    "                disease.append(int(datapoint['leaf_spot']))\n",
    "                disease.append(int(datapoint['curl']))\n",
    "\n",
    "                severity = []\n",
    "                for i in range(5):\n",
    "                    value = datapoint[f'severity_{i}']\n",
    "                    if value.lower() == 'not estimable':\n",
    "                        severity.append(self.imputation_value)\n",
    "                    else:\n",
    "                        severity.append(int(value))\n",
    "\n",
    "                self.data.append((datapoint['filename'], disease, severity))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, disease, severity = self.data[idx]\n",
    "\n",
    "        image_path = None\n",
    "        for subfolder in ['curl', 'healthy', 'slug', 'spot']:\n",
    "            potential_path = os.path.join(self.img_dir, subfolder, filename)\n",
    "            if os.path.exists(potential_path):\n",
    "                image_path = potential_path\n",
    "                break\n",
    "\n",
    "        if image_path is None:\n",
    "            raise Exception(f\"Image not found: {filename}\")\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "        image = Image.fromarray(image) \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        disease_label = torch.tensor(disease, dtype=torch.int)\n",
    "        severity_label = torch.tensor(severity, dtype=torch.int)\n",
    "\n",
    "        return image, disease_label, severity_label\n",
    "\n",
    "\n",
    "class DiaMOSDataset_Cartesian(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, data_path, transform=None, imputation_value=-1):\n",
    "        self.data = []\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.imputation_value = imputation_value\n",
    "        self.label_mapping = {}  \n",
    "        self.label_counter = 0\n",
    "\n",
    "        csv_file_path = os.path.join(data_path, 'annotation/csv', csv_file) \n",
    "        \n",
    "        with open(csv_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            header = lines[0].strip().split(';')\n",
    "            for line in lines[1:]:\n",
    "                datapoint = dict(zip(header, line.strip().split(';')))\n",
    "\n",
    "                disease = np.argmax([int(datapoint['healthy']), int(datapoint['pear_slug']), \n",
    "                                     int(datapoint['leaf_spot']), int(datapoint['curl'])])\n",
    "\n",
    "                severity = []\n",
    "                for i in range(5):\n",
    "                    value = datapoint[f'severity_{i}']\n",
    "                    if value.lower() == 'not estimable':\n",
    "                        severity.append(self.imputation_value)\n",
    "                    else:\n",
    "                        severity.append(int(value))\n",
    "                severity = np.argmax(severity)\n",
    "\n",
    "                combined_label = (disease, severity)\n",
    "                if combined_label not in self.label_mapping:\n",
    "                    self.label_mapping[combined_label] = self.label_counter\n",
    "                    self.label_counter += 1\n",
    "\n",
    "                self.data.append((datapoint['filename'], self.label_mapping[combined_label]))\n",
    "\n",
    "        # Create the reverse mapping\n",
    "        self.reverse_label_mapping = {v: k for k, v in self.label_mapping.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, combined_label = self.data[idx]\n",
    "\n",
    "        image_path = None\n",
    "        for subfolder in ['curl', 'healthy', 'slug', 'spot']:\n",
    "            potential_path = os.path.join(self.img_dir, subfolder, filename)\n",
    "            if os.path.exists(potential_path):\n",
    "                image_path = potential_path\n",
    "                break\n",
    "\n",
    "        if image_path is None:\n",
    "            raise Exception(f\"Image not found: {filename}\")\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "        image = Image.fromarray(image) \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, combined_label\n",
    "    \n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sanity_check_datasets():\n",
    "    # For DiaMOSDataset\n",
    "    dataset = DiaMOSDataset(csv_file='diaMOSPlant.csv', img_dir='data\\\\Pear\\\\leaves\\\\', data_path='data\\\\Pear\\\\')\n",
    "    print(len(dataset))\n",
    "    \n",
    "    for i in range(10):\n",
    "        image, disease_label, severity_label = dataset[i]\n",
    "        print(f\"Disease label: {disease_label}, Severity label: {severity_label}\")\n",
    "\n",
    "    # For DiaMOSDataset_Cartesian\n",
    "    dataset = DiaMOSDataset_Cartesian(csv_file='diaMOSPlant.csv', img_dir='data\\\\Pear\\\\leaves\\\\', data_path='data\\\\Pear\\\\')\n",
    "    print(len(dataset))\n",
    "    for i in range(10):\n",
    "        image, combined_label = dataset[i]\n",
    "        print(f\"Combined label: {combined_label}\")\n",
    "\n",
    "    disease_labels = [\"Healthy\", \"Pear Slug\", \"Leaf Spot\", \"Curl\"]\n",
    "    severity_labels = [\"None\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    "\n",
    "    # For DiaMOSDataset_Cartesian\n",
    "    dataset = DiaMOSDataset_Cartesian(csv_file='diaMOSPlant.csv', img_dir='data\\\\Pear\\\\leaves\\\\', data_path='data\\\\Pear\\\\')\n",
    "    print(len(dataset))\n",
    "    for i in range(10):\n",
    "        image, combined_label = dataset[i]\n",
    "        disease_label = disease_labels[combined_label // len(severity_labels)]\n",
    "        severity_label = severity_labels[combined_label % len(severity_labels)]\n",
    "        print(f\"Disease label: {disease_label}, Severity label: {severity_label}\")\n",
    "\n",
    "    import collections\n",
    "\n",
    "    # Get the labels\n",
    "    labels = [label for _, label in dataset]\n",
    "\n",
    "    # Count the labels\n",
    "    label_counts = collections.Counter(labels)\n",
    "\n",
    "    # Print the label counts\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} instances\")\n",
    "\n",
    "\n",
    "\n",
    "#### --------------------------------------------\n",
    "#### separate_classifiers.py\n",
    "\n",
    "def run_experiment_separate(experiment):\n",
    "    pass\n",
    "\n",
    "\n",
    "#### --------------------------------------------\n",
    "#### combined_classifier.py\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "\n",
    "class DiseaseSeverityModel_CombinedVGG(nn.Module):\n",
    "    def __init__(self, num_combined_labels):\n",
    "        super(DiseaseSeverityModel_CombinedVGG, self).__init__()\n",
    "        vgg19 = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
    "        for param in vgg19.features.parameters(): param.requires_grad = False\n",
    "        self.features = vgg19.features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_combined_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DiseaseSeverityModel_CombinedResNet50(nn.Module):\n",
    "    def __init__(self, num_combined_labels):\n",
    "        super(DiseaseSeverityModel_CombinedResNet50, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        for param in resnet50.parameters(): param.requires_grad = False\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_combined_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_combined(args, train_data, val_data, test_data, device, run, reverse_label_mapping):    \n",
    "    num_combined_labels = 20  # Adjust this according to your data\n",
    "    model = args.get(\"model\", DiseaseSeverityModel_CombinedVGG(num_combined_labels))\n",
    "    model = model.to(device)\n",
    "    run.watch(model)\n",
    "\n",
    "    criterion = args.get(\"criterion\", nn.CrossEntropyLoss())  \n",
    "    optimizer = args.get(\"optimizer\", torch.optim.Adam(model.parameters(), lr=args.get(\"lr\", 0.001)))\n",
    "    avg_val_loss = 0\n",
    "\n",
    "    # add a scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    epochs = args.get(\"epochs\", 10)\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=4, verbose=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "        print('Training...')\n",
    "        \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Put the model into training mode. Don't be mislead--the call to\n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n",
    "        model.train()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_accuracy_disease = 0\n",
    "        total_train_accuracy_severity = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_data):\n",
    "            # Progress update every 4 batches\n",
    "            if step % 4 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_data), elapsed))\n",
    "            \n",
    "            images = batch[0].to(device)\n",
    "            combined_labels = batch[1].to(device)  # Assuming combined labels are at index 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            combined_output = model(images)\n",
    "            \n",
    "            loss = criterion(combined_output, combined_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            logits_combined = combined_output.detach().cpu().numpy()\n",
    "            combined_label_ids = combined_labels.to('cpu').numpy()\n",
    "            \n",
    "            # Get disease and severity labels\n",
    "            labels_disease, labels_severity = zip(*[reverse_label_mapping[label] for label in combined_label_ids])\n",
    "            labels_disease = torch.tensor(labels_disease).to(device)\n",
    "            labels_severity = torch.tensor(labels_severity).to(device)\n",
    "\n",
    "            # Convert labels to one-hot encoding\n",
    "            labels_disease_one_hot = one_hot_encode(labels_disease.cpu().numpy(), num_combined_labels)\n",
    "            labels_severity_one_hot = one_hot_encode(labels_severity.cpu().numpy(), num_combined_labels)\n",
    "\n",
    "            # Split the combined logits into disease and severity parts\n",
    "            half_dim = logits_combined.shape[1] // 2\n",
    "            logits_disease, logits_severity = logits_combined[:, :half_dim], logits_combined[:, half_dim:]\n",
    "\n",
    "            total_train_accuracy_disease += flat_accuracy(logits_disease, labels_disease_one_hot)\n",
    "            total_train_accuracy_severity += flat_accuracy(logits_severity, labels_severity_one_hot)\n",
    "\n",
    "        avg_train_accuracy_disease = total_train_accuracy_disease / len(train_data)\n",
    "        avg_train_accuracy_severity = total_train_accuracy_severity / len(train_data)\n",
    "        print(\" Train Accuracy Disease: {0:.2f}\".format(avg_train_accuracy_disease))\n",
    "        print(\" Train Accuracy Severity: {0:.2f}\".format(avg_train_accuracy_severity))\n",
    "\n",
    "        run.log({\n",
    "            \"loss\": running_loss / len(train_data),\n",
    "            \"train_accuracy_disease\": avg_train_accuracy_disease,\n",
    "            \"train_accuracy_severity\": avg_train_accuracy_severity,\n",
    "        })\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        total_val_accuracy_disease = 0\n",
    "        total_val_accuracy_severity = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for _, batch in enumerate(val_data):   \n",
    "                \n",
    "            images = batch[0].to(device)\n",
    "            combined_labels = batch[1].to(device)  # Assuming combined labels are at index 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                combined_output = model(images)\n",
    "                \n",
    "            loss = criterion(combined_output, combined_labels)\n",
    "            avg_val_loss += loss.item() / len(val_data)\n",
    "\n",
    "            logits_combined = combined_output.detach().cpu().numpy()\n",
    "            combined_label_ids = combined_labels.to('cpu').numpy()\n",
    "            \n",
    "            # Get disease and severity labels\n",
    "            labels_disease, labels_severity = zip(*[reverse_label_mapping[label] for label in combined_label_ids])\n",
    "            labels_disease = torch.tensor(labels_disease).to(device)\n",
    "            labels_severity = torch.tensor(labels_severity).to(device)\n",
    "            \n",
    "            # Convert labels to one-hot encoding\n",
    "            labels_disease_one_hot = one_hot_encode(labels_disease.cpu().numpy(), num_combined_labels)\n",
    "            labels_severity_one_hot = one_hot_encode(labels_severity.cpu().numpy(), num_combined_labels)\n",
    "\n",
    "            # Split the combined logits into disease and severity parts\n",
    "            half_dim = logits_combined.shape[1] // 2\n",
    "            logits_disease, logits_severity = logits_combined[:, :half_dim], logits_combined[:, half_dim:]\n",
    "\n",
    "            total_val_accuracy_disease += flat_accuracy(logits_disease, labels_disease_one_hot)\n",
    "            total_val_accuracy_severity += flat_accuracy(logits_severity, labels_severity_one_hot)\n",
    "        \n",
    "        avg_val_accuracy_disease = total_val_accuracy_disease / len(val_data)\n",
    "        avg_val_accuracy_severity = total_val_accuracy_severity / len(val_data)\n",
    "        print(\" Validation Accuracy Disease: {0:.2f}\".format(avg_val_accuracy_disease))\n",
    "        print(\" Validation Accuracy Severity: {0:.2f}\".format(avg_val_accuracy_severity))\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        run.log({\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy_disease\": avg_val_accuracy_disease,\n",
    "            \"val_accuracy_severity\": avg_val_accuracy_severity,\n",
    "        })\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Load the best state dictionary into a new model\n",
    "    best_model = args.get(\"model\", DiseaseSeverityModel_CombinedVGG(num_combined_labels))\n",
    "    best_model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def run_experiment_combined(args):\n",
    "\n",
    "    # ------------------------------\n",
    "    # ----- Data Preparation -------\n",
    "    # ------------------------------\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    transformations = args.transforms if hasattr(args, 'transforms') else transforms.Compose([])\n",
    "    \n",
    "    name = args.name if hasattr(args, 'name') else \"combined_classifier\"\n",
    "\n",
    "    full_dataset = DiaMOSDataset_Cartesian(csv_file=args.csv_file if hasattr(args, 'csv_file') else 'diaMOSPlant.csv', \n",
    "                                 img_dir=args.img_dir if hasattr(args, 'img_dir') else '/kaggle/input/diamos-plant-dataset/Pear/leaves', \n",
    "                                 data_path=args.data_path if hasattr(args, 'data_path') else '/kaggle/input/diamos-plant-dataset/Pear/', \n",
    "                                 transform=transformations)\n",
    "    \n",
    "    train_size = args.train_size if hasattr(args, 'train_size') else int(0.6 * len(full_dataset))\n",
    "    val_size = args.val_size if hasattr(args, 'val_size') else int(0.2 * len(full_dataset))\n",
    "    test_size = args.test_size if hasattr(args, 'test_size') else len(full_dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "    batch_size = args.batch_size if hasattr(args, 'batch_size') else 16\n",
    "    num_workers = args.num_workers if hasattr(args, 'num_workers') else 2\n",
    "\n",
    "    train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_data = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_data = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # ----- Model Training ---------\n",
    "    # ------------------------------\n",
    "\n",
    "    args_dict = vars(args)\n",
    "    project_name = args.project_name if hasattr(args, 'project_name') else \"plant_disease_detection\"\n",
    "    run_name = name + \"_\" + animal_version_name(project_name)\n",
    "    print(\"Project Name:\", project_name)\n",
    "    run = wandb.init(name=run_name, reinit=True, entity=\"plant_disease_detection\", project=project_name)\n",
    "    run.config.update(args_dict)\n",
    "    model = train_combined(args_dict, train_data, val_data, test_data, device, run, full_dataset.reverse_label_mapping)\n",
    "\n",
    "    # ------------------------------\n",
    "    # ----- Model Evaluation -------\n",
    "    # ------------------------------\n",
    "\n",
    "    # Evaluation loop (for validation or test data)\n",
    "    model.eval()\n",
    "\n",
    "    # Add code for evaluation here (using validation or test data)\n",
    "    total_disease_correct = 0\n",
    "    total_severity_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize lists to store correctly classified images (to print them in the next cell)\n",
    "    correct_images = []\n",
    "    correct_disease_labels = []\n",
    "    correct_severity_labels = []\n",
    "    correct_disease_predictions = []\n",
    "    correct_severity_predictions = []\n",
    "\n",
    "    all_disease_labels = []\n",
    "    all_disease_predictions = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_data:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            \n",
    "            # Split labels into disease and severity\n",
    "            disease_labels = labels // 4  # Assuming there are 4 severity levels\n",
    "            severity_labels = labels % 4  # Assuming there are 4 severity levels\n",
    "\n",
    "            # Classification\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            disease_predicted = predicted // 4  # Assuming there are 4 severity levels\n",
    "            severity_predicted = predicted % 4  # Assuming there are 4 severity levels\n",
    "\n",
    "            disease_correct = (disease_predicted == disease_labels)\n",
    "            total_disease_correct += disease_correct.sum().item()\n",
    "\n",
    "            severity_correct = (severity_predicted == severity_labels)\n",
    "            total_severity_correct += severity_correct.sum().item()\n",
    "            \n",
    "            total_samples += images.size(0)\n",
    "\n",
    "            # Find indices of correctly classified images\n",
    "            correct_indices = (disease_correct & severity_correct).nonzero().squeeze()\n",
    "\n",
    "            # Check if correct_indices is a scalar\n",
    "            if correct_indices.dim() == 0:\n",
    "                correct_indices = [correct_indices.item()]\n",
    "\n",
    "            # Append correctly classified images and their labels/predictions\n",
    "            correct_images.extend(images[correct_indices].cpu().numpy())\n",
    "            correct_disease_labels.extend(disease_labels[correct_indices].cpu().numpy())\n",
    "            correct_severity_labels.extend(severity_labels[correct_indices].cpu().numpy())\n",
    "            correct_disease_predictions.extend(disease_predicted[correct_indices].cpu().numpy())\n",
    "            correct_severity_predictions.extend(severity_predicted[correct_indices].cpu().numpy())\n",
    "\n",
    "            # Append all disease labels and predictions\n",
    "            all_disease_labels.extend(disease_labels.cpu().numpy())\n",
    "            all_disease_predictions.extend(disease_predicted.cpu().numpy())\n",
    "            all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "            all_severity_predictions.extend(severity_predicted.cpu().numpy())\n",
    "\n",
    "    disease_accuracy = total_disease_correct / total_samples\n",
    "    severity_accuracy = total_severity_correct / total_samples\n",
    "\n",
    "    # Compute precision, recall, and accuracy per label for disease classification\n",
    "    disease_report = classification_report(all_disease_labels, all_disease_predictions, output_dict=True)\n",
    "\n",
    "    # Compute precision, recall, and accuracy per label for severity classification\n",
    "    severity_report = classification_report(all_severity_labels, all_severity_predictions, output_dict=True)\n",
    "\n",
    "    print(f'Disease Classification Accuracy: {disease_accuracy * 100:.2f}%')\n",
    "    print(f'Severity Classification Accuracy: {severity_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    run.log({\n",
    "        \"test_accuracy_disease\": disease_accuracy,\n",
    "        \"test_accuracy_severity\": severity_accuracy,\n",
    "        \"disease_report\": disease_report,\n",
    "        \"severity_report\": severity_report,\n",
    "    })\n",
    "\n",
    "    info = {\n",
    "        \"run_name\": run_name,\n",
    "        \"hyperparameters\": args_dict,\n",
    "        \"accuracy_disease\": disease_accuracy,\n",
    "        \"accuracy_severity\": severity_accuracy,\n",
    "        \"disease_report\": disease_report,\n",
    "        \"severity_report\": severity_report,\n",
    "    }\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"disease_severity_model.pth\")\n",
    "    run.save(\"disease_severity_model.pth\")\n",
    "\n",
    "    with open(\"model_info.txt\", \"w\") as f:\n",
    "        f.write(json.dumps(info, indent=4))\n",
    "\n",
    "    wandb.join()\n",
    "\n",
    "\n",
    "#### --------------------------------------------\n",
    "#### divergent_heads_classifier.py\n",
    "\n",
    "\n",
    "\n",
    "class DiseaseSeverityModelVGG(nn.Module):\n",
    "    def __init__(self, num_disease_classes, num_severity_levels):\n",
    "        super(DiseaseSeverityModelVGG, self).__init__()\n",
    "        vgg19 = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
    "        for param in vgg19.features.parameters(): param.requires_grad = False\n",
    "        self.features = vgg19.features\n",
    "        self.avgpool = vgg19.avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.disease_classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, num_disease_classes),\n",
    "        )\n",
    "        self.severity_classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, num_severity_levels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        disease_output = self.disease_classifier(x)\n",
    "        severity_output = self.severity_classifier(x)\n",
    "        return disease_output, severity_output\n",
    "    \n",
    "\n",
    "class DiseaseSeverityModelResNet50(nn.Module):\n",
    "    def __init__(self, num_disease_classes, num_severity_levels):\n",
    "        super(DiseaseSeverityModelResNet50, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        for param in resnet50.parameters(): param.requires_grad = False\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.disease_classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, num_disease_classes),\n",
    "        )\n",
    "        self.severity_classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, num_severity_levels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        disease_output = self.disease_classifier(x)\n",
    "        severity_output = self.severity_classifier(x)\n",
    "        return disease_output, severity_output\n",
    "\n",
    "\n",
    "def train_divergent(args, train_data, val_data, test_data, device, run):    \n",
    "    model = args.get(\"model\", DiseaseSeverityModelVGG(num_disease_classes=4, num_severity_levels=5))\n",
    "    model = model.to(device)\n",
    "    run.watch(model)\n",
    "\n",
    "    criterion = args.get(\"criterion\", nn.CrossEntropyLoss())  \n",
    "    optimizer = args.get(\"optimizer\", torch.optim.Adam(model.parameters(), lr=args.get(\"lr\", 0.001)))\n",
    "    avg_val_loss = 0\n",
    "\n",
    "    # add a scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    epochs = args.get(\"epochs\", 10)\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "\n",
    "    total_t0 = time.time()\n",
    "    early_stopping = EarlyStopping(patience=4, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "        print('Training...')\n",
    "        \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Put the model into training mode. Don't be mislead--the call to\n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questimport gensim.downloader as api\n",
    "        model.train()\n",
    "        \n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_accuracy_disease = 0\n",
    "        total_train_accuracy_severity = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # for images, disease_labels, severity_labels in train_data:\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_data):\n",
    "            \n",
    "            # Progress update every 4 batches\n",
    "            if step % 4 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_data), elapsed))\n",
    "            \n",
    "            images = batch[0].to(device)\n",
    "            disease_labels = batch[1].to(device)\n",
    "            severity_labels = batch[2].to(device)\n",
    "            # images, disease_labels, severity_labels = images.to(device), disease_labels.to(device), severity_labels.to(device)\n",
    "            \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because\n",
    "            # accumulating the gradients is \"convenient while training RNNs\".\n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            disease_output, severity_output = model(images)\n",
    "            \n",
    "            disease_loss = criterion(disease_output, torch.argmax(disease_labels, dim=1))\n",
    "            severity_loss = criterion(severity_output, torch.argmax(severity_labels, dim=1))\n",
    "            \n",
    "            loss = disease_loss + severity_loss\n",
    "            run.log({\"loss\": loss.item(), \"val_loss\": avg_val_loss})\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            logits_disease = disease_output.detach().cpu().numpy()\n",
    "            logits_severity = severity_output.detach().cpu().numpy()\n",
    "            \n",
    "            disease_label_ids = disease_labels.to('cpu').numpy()\n",
    "            severity_label_ids = severity_labels.to('cpu').numpy()\n",
    "            \n",
    "            total_train_accuracy_disease += flat_accuracy(logits_disease, disease_label_ids)\n",
    "            total_train_accuracy_severity += flat_accuracy(logits_severity, severity_label_ids)\n",
    "\n",
    "\n",
    "        avg_train_accuracy_disease = total_train_accuracy_disease / len(train_data)\n",
    "        print(\" Train Accuracy - Disease: {0:.2f}\".format(avg_train_accuracy_disease))\n",
    "            \n",
    "        avg_train_accuracy_severity = total_train_accuracy_severity / len(train_data)\n",
    "        print(\" Train Accuracy - Severity: {0:.2f}\".format(avg_train_accuracy_severity))\n",
    "        \n",
    "        run.log({\n",
    "            \"loss\": running_loss / len(train_data),\n",
    "            \"train_accuracy_disease\": avg_train_accuracy_disease,\n",
    "            \"train_accuracy_severity\": avg_train_accuracy_severity,\n",
    "        })\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        \n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        # for images, disease_labels, severity_labels in val_data:\n",
    "        # For each batch of training data...\n",
    "\n",
    "        total_val_accuracy_disease = 0\n",
    "        total_val_accuracy_severity = 0\n",
    "\n",
    "        for _, batch in enumerate(val_data):   \n",
    "                \n",
    "            images = batch[0].to(device)\n",
    "            disease_labels = batch[1].to(device)\n",
    "            severity_labels = batch[2].to(device)\n",
    "            \n",
    "            # images, disease_labels, severity_labels = images.to(device), disease_labels.to(device), severity_labels.to(device)\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():\n",
    "                disease_output, severity_output = model(images)\n",
    "                \n",
    "            disease_loss = criterion(disease_output, torch.argmax(disease_labels, dim=1))\n",
    "            severity_loss = criterion(severity_output, torch.argmax(severity_labels, dim=1))\n",
    "            val_loss += (disease_loss + severity_loss).item()\n",
    "\n",
    "            logits_disease = disease_output.detach().cpu().numpy()\n",
    "            logits_severity = severity_output.detach().cpu().numpy()\n",
    "            \n",
    "            disease_label_ids = disease_labels.to('cpu').numpy()\n",
    "            severity_label_ids = severity_labels.to('cpu').numpy()\n",
    "            \n",
    "            total_val_accuracy_disease += flat_accuracy(logits_disease, disease_label_ids)\n",
    "            total_val_accuracy_severity += flat_accuracy(logits_severity, severity_label_ids)\n",
    "\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_data)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_data)}, Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "        avg_val_accuracy_disease = total_val_accuracy_disease / len(val_data)\n",
    "        avg_val_accuracy_severity = total_val_accuracy_severity / len(val_data)\n",
    "\n",
    "        print(\" Validation Accuracy - Disease: {0:.2f}\".format(avg_val_accuracy_disease))\n",
    "        print(\" Validation Accuracy - Severity: {0:.2f}\".format(avg_val_accuracy_severity))\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        run.log({\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy_disease\": avg_val_accuracy_disease,\n",
    "            \"val_accuracy_severity\": avg_val_accuracy_severity,\n",
    "        })\n",
    "        \n",
    "        # Check if validation loss has improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"disease_severity_model.pth\")  # Save the model\n",
    "            run.save(\"disease_severity_model.pth\")\n",
    "\n",
    "        # Perform early stopping\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(\"disease_severity_model.pth\"))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_experiment_divergent(args):\n",
    "    \n",
    "    # ------------------------------\n",
    "    # ----- Data Preparation -------\n",
    "    # ------------------------------\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    transformations = args.transforms if hasattr(args, 'transforms') else transforms.Compose([])\n",
    "    \n",
    "    name = args.name if hasattr(args, 'name') else \"divergent_heads_classifier\"\n",
    "\n",
    "    full_dataset = DiaMOSDataset(csv_file=args.csv_file if hasattr(args, 'csv_file') else 'diaMOSPlant.csv', \n",
    "                                 img_dir=args.img_dir if hasattr(args, 'img_dir') else '/kaggle/input/diamos-plant-dataset/Pear/leaves', \n",
    "                                 data_path=args.data_path if hasattr(args, 'data_path') else '/kaggle/input/diamos-plant-dataset/Pear/', \n",
    "                                 transform=transformations)\n",
    "\n",
    "    train_size = args.train_size if hasattr(args, 'train_size') else int(0.6 * len(full_dataset))\n",
    "    val_size = args.val_size if hasattr(args, 'val_size') else int(0.2 * len(full_dataset))\n",
    "    test_size = args.test_size if hasattr(args, 'test_size') else len(full_dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "    batch_size = args.batch_size if hasattr(args, 'batch_size') else 16\n",
    "    num_workers = args.num_workers if hasattr(args, 'num_workers') else 2\n",
    "\n",
    "    train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_data = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_data = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # ----- Model Training ---------\n",
    "    # ------------------------------\n",
    "\n",
    "    args_dict = vars(args)\n",
    "    project_name = args.project_name if hasattr(args, 'project_name') else \"plant_disease_detection\"\n",
    "    run_name = name + \"_\" + animal_version_name(project_name)\n",
    "    print(\"Project Name:\", project_name)\n",
    "    run = wandb.init(name=run_name, reinit=True, entity=\"plant_disease_detection\", project=project_name)\n",
    "    run.config.update(args_dict)\n",
    "    model = train_divergent(args_dict, train_data, val_data, test_data, device, run)\n",
    "\n",
    "    # ------------------------------\n",
    "    # ----- Model Evaluation -------\n",
    "    # ------------------------------\n",
    "\n",
    "    # Evaluation loop (for validation or test data)\n",
    "    model.eval()\n",
    "\n",
    "    # Add code for evaluation here (using validation or test data)\n",
    "    total_disease_correct = 0\n",
    "    total_severity_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize lists to store correctly classified images (to print them in the next cell)\n",
    "    correct_images = []\n",
    "    correct_disease_labels = []\n",
    "    correct_severity_labels = []\n",
    "    correct_disease_predictions = []\n",
    "    correct_severity_predictions = []\n",
    "\n",
    "    all_disease_labels = []\n",
    "    all_disease_predictions = []\n",
    "    all_severity_labels = []\n",
    "    all_severity_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, disease_labels, severity_labels in test_data:\n",
    "            images, disease_labels, severity_labels = images.to(device), disease_labels.to(device), severity_labels.to(device)\n",
    "            disease_output, severity_output = model(images)\n",
    "            \n",
    "            # Disease classification\n",
    "            _, disease_predicted = torch.max(disease_output, 1)\n",
    "            disease_correct = (disease_predicted == torch.argmax(disease_labels, dim=1))\n",
    "            total_disease_correct += disease_correct.sum().item()\n",
    "            \n",
    "            # Severity classification\n",
    "            _, severity_predicted = torch.max(severity_output, 1)\n",
    "            severity_correct = (severity_predicted == torch.argmax(severity_labels, dim=1))\n",
    "            total_severity_correct += severity_correct.sum().item()\n",
    "            \n",
    "            total_samples += images.size(0)\n",
    "\n",
    "            # Find indices of correctly classified images\n",
    "            correct_indices = (disease_correct & severity_correct).nonzero().squeeze()\n",
    "\n",
    "            # Check if correct_indices is a scalar\n",
    "            if correct_indices.dim() == 0:\n",
    "                correct_indices = [correct_indices.item()]\n",
    "\n",
    "            # Append correctly classified images and their labels/predictions\n",
    "            correct_images.extend(images[correct_indices].cpu().numpy())\n",
    "            correct_disease_labels.extend(disease_labels[correct_indices].cpu().numpy())\n",
    "            correct_severity_labels.extend(severity_labels[correct_indices].cpu().numpy())\n",
    "            correct_disease_predictions.extend(disease_predicted[correct_indices].cpu().numpy())\n",
    "            correct_severity_predictions.extend(severity_predicted[correct_indices].cpu().numpy())\n",
    "\n",
    "            # Append all disease labels and predictions\n",
    "            all_disease_labels.extend(torch.argmax(disease_labels, dim=1).cpu().numpy())\n",
    "            all_disease_predictions.extend(disease_predicted.cpu().numpy())\n",
    "            all_severity_labels.extend(torch.argmax(severity_labels, dim=1).cpu().numpy())\n",
    "            all_severity_predictions.extend(severity_predicted.cpu().numpy())\n",
    "\n",
    "    disease_accuracy = total_disease_correct / total_samples\n",
    "    severity_accuracy = total_severity_correct / total_samples\n",
    "\n",
    "    # Compute precision, recall, and accuracy per label for disease classification\n",
    "    disease_report = classification_report(all_disease_labels, all_disease_predictions, output_dict=True)\n",
    "\n",
    "    # Compute precision, recall, and accuracy per label for severity classification\n",
    "    severity_report = classification_report(all_severity_labels, all_severity_predictions, output_dict=True)\n",
    "\n",
    "    print(f'Disease Classification Accuracy: {disease_accuracy * 100:.2f}%')\n",
    "    print(f'Severity Classification Accuracy: {severity_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    run.log({\n",
    "        \"test_accuracy_disease\": disease_accuracy,\n",
    "        \"test_accuracy_severity\": severity_accuracy,\n",
    "        \"disease_report\": disease_report,\n",
    "        \"severity_report\": severity_report,\n",
    "    })\n",
    "\n",
    "    info = {\n",
    "        \"run_name\": run_name,\n",
    "        \"hyperparameters\": args_dict,\n",
    "        \"accuracy_disease\": disease_accuracy,\n",
    "        \"accuracy_severity\": severity_accuracy,\n",
    "        \"disease_report\": disease_report,\n",
    "        \"severity_report\": severity_report,\n",
    "    }\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"disease_severity_model.pth\")\n",
    "    run.save(\"disease_severity_model.pth\")\n",
    "\n",
    "    with open(\"model_info.txt\", \"w\") as f:\n",
    "        f.write(json.dumps(info, indent=4))\n",
    "\n",
    "    wandb.join()\n",
    "\n",
    "\n",
    "\n",
    "#### --------------------------------------------\n",
    "#### main.py\n",
    "\n",
    "EXPERIMENT1 = SimpleNamespace(\n",
    "    classifier = \"divergent_heads\",\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # img_dir = 'data\\\\Pear\\\\leaves\\\\',\n",
    "    # data_path = 'data\\\\Pear\\\\',\n",
    "    name = \"divergent_heads\",\n",
    "    project_name = \"Teo Runs\",\n",
    "    epochs = 25,\n",
    ")\n",
    "\n",
    "EXPERIMENT2 = SimpleNamespace(\n",
    "    classifier = \"divergent_heads\",\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    # img_dir = 'data\\\\Pear\\\\leaves\\\\',\n",
    "    # data_path = 'data\\\\Pear\\\\',\n",
    "    name = \"divergent_heads_no_normalization\",\n",
    "    project_name = \"Teo Runs\",\n",
    "    epochs = 25,\n",
    ")\n",
    "\n",
    "EXPERIMENT3 = SimpleNamespace(\n",
    "    classifier = \"combined\",\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # img_dir = 'data\\\\Pear\\\\leaves\\\\',\n",
    "    # data_path = 'data\\\\Pear\\\\',\n",
    "    name = 'combined',\n",
    "    project_name = \"Teo Runs\",\n",
    "    epochs = 25,\n",
    ")\n",
    "\n",
    "EXPERIMENT4 = SimpleNamespace(\n",
    "    classifier = \"combined\",\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # img_dir = 'data\\\\Pear\\\\leaves\\\\',\n",
    "    # data_path = 'data\\\\Pear\\\\',\n",
    "    name = 'combined_no_normalization',\n",
    "    project_name = \"Teo Runs\",\n",
    "    epochs = 25,\n",
    ")\n",
    "\n",
    "EXPERIMENT5 = SimpleNamespace(\n",
    "    classifier = \"divergent_heads\",\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # img_dir = 'data\\\\Pear\\\\leaves\\\\',\n",
    "    # data_path = 'data\\\\Pear\\\\',\n",
    "    name = 'divergent_heads_resnet',\n",
    "    project_name = \"Teo Runs\",\n",
    "    epochs = 25,\n",
    "    model = DiseaseSeverityModelResNet50(num_disease_classes=4, num_severity_levels=5)\n",
    ")\n",
    "\n",
    "EXPERIMENT6 = SimpleNamespace(\n",
    "    classifier = \"combined\",\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # img_dir = 'data\\\\Pear\\\\leaves\\\\',\n",
    "    # data_path = 'data\\\\Pear\\\\',\n",
    "    name = 'combined_resnet',\n",
    "    project_name = \"Teo Runs\",\n",
    "    epochs = 25,\n",
    "    model = DiseaseSeverityModel_CombinedResNet50(20)\n",
    ")\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    EXPERIMENT1,\n",
    "    EXPERIMENT2,\n",
    "    EXPERIMENT3,\n",
    "    EXPERIMENT4,\n",
    "    EXPERIMENT5,\n",
    "    EXPERIMENT6,\n",
    "]\n",
    "\n",
    "\n",
    "def run_experiments(experiments):\n",
    "    for experiment in experiments:\n",
    "        if experiment.classifier == \"separate\":\n",
    "            run_experiment_separate(experiment)\n",
    "        elif experiment.classifier == \"combined\":\n",
    "            run_experiment_combined(experiment)\n",
    "        elif experiment.classifier == \"divergent_heads\":\n",
    "            run_experiment_divergent(experiment)\n",
    "        else:\n",
    "            print(\"Invalid classifier type\")\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "wandb.login(key=\"a8acb651e87c4dca872eeb0bdedcfccf93ab7171\")\n",
    "run_experiments(EXPERIMENTS)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
